# Chat API Design (Phase 1)

## 1. Product scope & user stories

- **Single-turn answer**: As an end user, I send a question and receive an answer generated by Gemini that is grounded on relevant documents retrieved from Chroma.
- **Contextual follow-up** (phase 2+): As an end user, I can ask a follow-up question that reuses previous context without re-uploading the entire conversation.
- **System configuration**: As an operator, I can tune RAG parameters (max context documents, temperature, safety toggles) via environment variables without code changes.
- **Observability**: As an operator, I can trace each response by a request ID and inspect timings of embedding, retrieval, and generation steps.

## 2. Workflow overview

1. Receive request payload (text query, optional conversation/session metadata).
2. Normalize and validate inputs (strip whitespace, optional language guardrails).
3. Generate embedding with `GeminiEmbeddingService`.
4. Query Chroma (`ChromaComponent`) for top _k_ matches, applying optional metadata filters.
5. Construct prompt/context (templated system prompt + concatenated excerpts + user query).
6. Call Gemini text-generation endpoint, using configured model/temperature/max tokens.
7. Post-process Gemini response (trim whitespace, optionally include citations/metadata).
8. Return JSON payload containing answer, metadata about retrieval, and request ID.

## 3. API surface

### 3.1 REST endpoints

- `POST /chat/query`

  - **Request body** (`ChatQueryRequest`):
    - `query: str` – user utterance (required).
    - `conversation_id: Optional[str]` – ties follow-ups to a conversation (optional).
    - `metadata_filters: Optional[dict[str, str | list[str]]]` – pass-through filters for Chroma (optional, validated against allow list).
    - `override: Optional[ChatParametersOverride]` – per-request overrides for temperature, `top_k`, `max_tokens`, `include_embeddings`.
  - **Response body** (`ChatQueryResponse`):
    - `conversation_id: str` – echoes or creates ID.
    - `answer: str` – formatted response from Gemini.
    - `sources: list[RetrievedDocument]` – subset of Chroma matches with IDs, score, metadata.
    - `usage: UsageMetrics` – tokens used, timings for embedding/retrieval/generation.
    - `request_id: str` – traceable identifier for logs.

- (Phase 2) `GET /chat/conversation/{conversation_id}` – fetch history/cached context.
- (Phase 2) WebSocket endpoint `/chat/stream` for incremental token streaming.

### 3.2 Data models (Pydantic v2)

```python
class ChatParametersOverride(BaseModel):
    temperature: float | None = Field(ge=0.0, le=2.0)
    top_k: int | None = Field(gt=0, le=50)
    max_output_tokens: int | None = Field(gt=0, le=8192)
    include_embeddings: bool | None = None

class ChatQueryRequest(BaseModel):
    query: str = Field(min_length=1)
    conversation_id: str | None = None
    metadata_filters: dict[str, Any] | None = None
    override: ChatParametersOverride | None = None

class RetrievedDocument(BaseModel):
    id: str
    text: str
    metadata: dict[str, Any] | None = None
    distance: float | None = None

class UsageMetrics(BaseModel):
    embedding_ms: float
    retrieval_ms: float
    generation_ms: float
    total_ms: float
    total_tokens: int | None = None

class ChatQueryResponse(BaseModel):
    conversation_id: str
    answer: str
    sources: list[RetrievedDocument]
    usage: UsageMetrics
    request_id: str
```

## 4. Configuration & environment variables

| Variable                          | Default              | Purpose                                          |
| --------------------------------- | -------------------- | ------------------------------------------------ |
| `CHAT_MODEL`                      | `gemini-2.5-flash`   | Default Gemini model for text generation.        |
| `CHAT_MAX_TOKENS`                 | `2048`               | Cap on output tokens for Gemini chat completion. |
| `CHAT_TEMPERATURE`                | `0.7`                | Temperature for creativity vs. determinism.      |
| `CHAT_MAX_CONTEXT_DOCS`           | `5`                  | Maximum documents taken from Chroma per request. |
| `CHAT_RESPONSE_FORMAT`            | `text`               | Reserved for future JSON/markdown format hints.  |
| `CHAT_ALLOWED_METADATA_KEYS`      | `` (comma-separated) | Restrict metadata filters to safe keys.          |
| `CHAT_DEFAULT_SYSTEM_PROMPT_PATH` | ``                   | Optional path to prompt template file.           |
| `CHAT_ENABLE_STREAMING`           | `false`              | Feature flag for WebSocket streaming.            |

All values flow through `ChatSettings` (new settings class) and can be overridden per environment. Overrides in requests are validated against safe ranges defined in the settings object.

## 5. Shared dependencies & packages

- **FastAPI** – primary HTTP framework (`fastapi>=0.115`).
- **Uvicorn** – ASGI server for local dev and production (`uvicorn[standard]>=0.30`).
- **httpx** – async HTTP client for API tests (`httpx>=0.27`).
- **python-dotenv** (optional) – developer convenience for loading `.env` when running via `uv run`.

Dependency strategy: add FastAPI + Uvicorn to core dependencies; add httpx to the `test` group. Keep versions coherent with Python 3.13 support.

## 6. Observability & logging plan

- Generate UUID `request_id` per request (use `uuid4()`); attach to log records using `logging.LoggerAdapter` or `contextvars`.
- Collect step-level durations; include them in response and logs.
- Configure structured JSON logging behind an env flag (phase 2) to integrate with external log pipelines.

## 7. Security & validation considerations

- Input sanitisation: strip control characters, enforce max query length (e.g., 2,000 chars).
- Metadata filter allow list to prevent arbitrary SQL-like injection into Chroma metadata queries.
- Optional API key header (`X-API-Key`) – to be enforced via dependency injection once implemented.
- Rate limiting (phase 3) – integrate with fastapi-limiter or front proxy.

## 8. Implementation milestones

1. **Settings & scaffolding**: add `ChatSettings`, env defaults, dependency additions (`pyproject.toml`).
2. **Service orchestrator**: implement core RAG flow (`ChatService`).
3. **API layer**: create FastAPI routes + main entrypoint.
4. **Testing & docs**: write unit/API tests, update README/usage guides.
5. **Advanced features**: session persistence, streaming, auth.

## 9. Open questions / follow-ups

- Do we need multilingual support (affects prompt templating and safety filters)?
- Should sources include confidence scores or explicit citations markup?
- Is streaming a launch requirement or can it be gated behind feature flag?
- Preferred authentication mechanism (API keys vs. OAuth vs. upstream gateway)?

This document captures the decisions needed for Phase 1. Later phases should reference and refine this plan as implementation details solidify.
